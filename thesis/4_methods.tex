\section{Methods}

\subsection{Data Analysis}

3d, mri, german males, 14-21, different resolutions, coronal/sagittal, segmentation maps for 76, mhd files, distribution of age.



\subsection{Preprocessing}

The number of parameters in a Neural Network commonly range from hundreds of thousands to hundreds of millions. This complexity allows the model to learn on its own what features of an image are relevant for any given task. It works in conjunction with the fact that high volumes of data are available for the training.

Because of the small dataset that was available for this study, several types of data preprocessing were applied to the images. These techniques reduce the amount of information per sample or the amount of variance between multiple samples. This results in a complexity reduction of the problem the network is supposed to solve. Other preprocessing methods experimented with the difference between 2D and 3D data as well as the influence of separate segmentation channels on the output.

\subsubsection{Cropping}

The framing of the raw images included large parts of the thigh and shin to be visible in the picture. Since these weren't relevant for the purpose of the study, they were cropped out. An algorithm was used to detect the center where Tibia and Femur meet and only use a square window around this point. There was no cropping applied on the z-axis.

\subsubsection{Resizing}

The images were also resized to a resolution that is common for Convolutional Neural Networks. 224x224 Pixels for width and height also allowed the use of Transfer Learning based on popular pre-trained models. This resolution still delivered enough detail for the segmentation maps.

On the z-axis everything was scaled to 36 slices, which meant a 1.5 upscale for the images provided by Jopp et al. and a minor downscale from the 41 slices that were taken specifically for this study.

Every image now had unified dimensions of 36x224x224 voxels.

\subsubsection{Normalization}

The normalization procedure of this dataset was executed in two steps. First the N4 Bias Correction was applied to the images, which tries to balance irregularities than happen when the MRIs were recorded. In the second step all intensity values were normalized to range from 0 to 1 so that every input is on the same scale.

\subsubsection{2D and 3D data}

Every three dimensional image can be converted to n two dimensional slices, where n is the resolution of the sliced axis. Since the z-axis shows a much lower resolution than x and y, each image was sliced in 36 224x224 2D images. This resulted in 36 times more samples, but reduced the information per image by the same factor. 

Using three dimensional convolutions turned out to be helpful for the segmentation, because the network was able to draw conclusions from the order of the slices within one image. 

\subsubsection{Separate Bone Maps}

The initial segmentation maps came with three separate channels for the Femur, Tibia and Fibula. With this information it was possible to train a model that would segment the three bones while still differentiating between them. This helped the accuracy of the prediction opposed to using just a single channel for all of the bones. In places where the Femur and Tibia were very close to one another, the separate channels prevented the closing of this region by the network.

For another experiment the three channels were treated as one to create a network that would segment any type of bone in the image. This resulted in better performance when applied to sagittal images of the knee provided by Maas et al. The network was able to generalize on a situation it wasn't trained on.

\subsection{Architectures}



In search for a network that would perform well on the segmentation, different architectures were looked at and multiple settings were tried.

\subsubsection{Patch and Image based}




\subsubsection{Channels}

growth and initial size

\subsubsection{Dropout}

Dropout is a popular regularization technique that randomly zeros out a fraction of the weights during training. It is understood that this helps the model to generalize better and reduce overfitting on a given dataset. Well known image classification architectures like VGG16, SqueezeNet or AlexNet use Dropout near the end of the network. Similarly U-Net uses Dropout at the end of the contracting path of the architecture to implicitly add image augmentation to the data. The original paper does not give any information how much Dropout was used.

Since overfitting was a big problem of this study, several other Dropout strategies were investigated. Increasing the rate above 0.5 did reduce the overfitting, but also hurt the performance of the model. Placing a Dropout unit between each convolution pair on the contracting side turned out to be an effective method for this dataset. Multiple amounts of Dropout were tested of which 0.2 achieved the best results. 0.1 resulted is more overfitting and 0.3 led to instabilities during training.

While it didn't hurt the performance when low amounts of Dropout were also added to the expanding side, it had a negative effect on the training speed. We believe this is due to the nature of an encoding/decoding architecture. On the contracting side of the network the amount of information is compressed by decreasing the spatial resolution of the images. This dense representation can lead to overfitting because the number of features is heavily reduced from the input. The upscaling side of the model expands the available information, which helps to fight overfitting on its own.


\subsubsection{Batch Size}

Neural Networks use a process called stochastic gradient descent (SGD) or one of its variants to approximate the gradient on a small fraction of the data. The size of this fraction is called the batch size and describes how many samples are used for a single forward- and back-propagation step.

In the past it was believed that larger batches led to something called the generalization gap (1609.04836), where the accuracy of a model would drop if it was trained on particularly large batches. Recent work by Hoffer at al. (1705.08741) suggests other reasons for this drop in accuracy. While common batch sizes range from 32 to 256, Goyal et al. showed accurate results using 8192 images per batch when training a model on imagenet (1706.02677).

Depending on the size of the input one may be restricted to smaller batches. In the field of 3D convolutions even one image can take up a majority of the RAM on a workstation.

\subsection{Training}

\subsubsection{Loss Function}

\subsubsection{Optimizer}

\subsubsection{Learning Rate Policy}

\subsubsection{Early Stopping}

IoU, Adam, early stopping, LR policy

\newpage
