@phdthesis{Alexander2015,
author = {Alexander, Markus and Mauer, Auf Der and Examiner, Second},
file = {:Users/pietz/Google Drive/Documents/Mendeley/masterThesis-AufderMauer.pdf:pdf},
title = {{Automated Quantification of the Growth Plate of the Proximal Tibia for the Age Assessment in 3D MR Images Using a Fuzzy-Logic Classification Approach}},
year = {2015}
}
@book{Chollet2017,
author = {Chollet, Francois},
booktitle = {Manning},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Deep{\_}Learning{\_}with{\_}Python{\_}v4{\_}MEAP.pdf:pdf},
keywords = {ai,artificial intelligence,deep learning,dl,keras,machine learning,ml},
mendeley-tags = {deep learning,dl,keras,artificial intelligence,ai,machine learning,ml},
title = {{Deep Learning With Python}},
volume = {1},
year = {2017}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Simonyan, Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
keywords = {classification,cnn,imagenet,vgg16},
mendeley-tags = {vgg16,imagenet,cnn,classification},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Ian Goodfellow, Yoshua Bengio, Aaron Courville Deep Learning.pdf:pdf},
isbn = {0262035618},
publisher = {The MIT Press},
title = {{Deep Learning}},
year = {2016}
}
@article{Milletari2016,
abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
archivePrefix = {arXiv},
arxivId = {1606.04797},
author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
eprint = {1606.04797},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Milletari, Navab, Ahmadi - 2016 - V-Net Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation.pdf:pdf},
month = {jun},
title = {{V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation}},
url = {http://arxiv.org/abs/1606.04797},
year = {2016}
}
@article{Kayalibay2017,
abstract = {Convolutional neural networks have been applied to a wide variety of computer vision tasks. Recent advances in semantic segmentation have enabled their application to medical image segmentation. While most CNNs use two-dimensional kernels, recent CNN-based publications on medical image segmentation featured three-dimensional kernels, allowing full access to the three-dimensional structure of medical images. Though closely related to semantic segmentation, medical image segmentation includes specific challenges that need to be addressed, such as the scarcity of labelled data, the high class imbalance found in the ground truth and the high memory demand of three-dimensional images. In this work, a CNN-based method with three-dimensional filters is demonstrated and applied to hand and brain MRI. Two modifications to an existing CNN architecture are discussed, along with methods on addressing the aforementioned challenges. While most of the existing literature on medical image segmentation focuses on soft tissue and the major organs, this work is validated on data both from the central nervous system as well as the bones of the hand.},
archivePrefix = {arXiv},
arxivId = {1701.03056},
author = {Kayalibay, Baris and Jensen, Grady and van der Smagt, Patrick},
eprint = {1701.03056},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Kayalibay, Jensen, van der Smagt - 2017 - CNN-based Segmentation of Medical Imaging Data.pdf:pdf},
month = {jan},
title = {{CNN-based Segmentation of Medical Imaging Data}},
url = {http://arxiv.org/abs/1701.03056},
year = {2017}
}
@article{Shelhamer2016,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30{\%} relative improvement to 67.2{\%} mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1605.06211},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
eprint = {1605.06211},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Shelhamer, Long, Darrell - 2016 - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
month = {may},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {http://arxiv.org/abs/1605.06211},
year = {2016}
}
@article{Wang2017,
abstract = {Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have led to significant improvement over previous semantic segmentation systems. Here we show how to improve pixel-wise semantic segmentation by manipulating convolution-related operations that are better for practical use. First, we implement dense upsampling convolution (DUC) to generate pixel-level prediction, which is able to capture and decode more detailed information that is generally missing in bilinear upsampling. Second, we propose a hybrid dilated convolution (HDC) framework in the encoding phase. This framework 1) effectively enlarges the receptive fields of the network to aggregate global information; 2) alleviates what we call the "gridding issue" caused by the standard dilated convolution operation. We evaluate our approaches thoroughly on the Cityscapes dataset, and achieve a new state-of-art result of 80.1{\%} mIOU in the test set. We also are state-of-the-art overall on the KITTI road estimation benchmark and the PASCAL VOC2012 segmentation task. Pretrained models are available at https://goo.gl/DQMeun},
archivePrefix = {arXiv},
arxivId = {1702.08502},
author = {Wang, Panqu and Chen, Pengfei and Yuan, Ye and Liu, Ding and Huang, Zehua and Hou, Xiaodi and Cottrell, Garrison},
eprint = {1702.08502},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Wang et al. - 2017 - Understanding Convolution for Semantic Segmentation.pdf:pdf},
month = {feb},
title = {{Understanding Convolution for Semantic Segmentation}},
url = {http://arxiv.org/abs/1702.08502},
year = {2017}
}
@article{Cicek2016,
abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.},
archivePrefix = {arXiv},
arxivId = {1606.06650},
author = {{\c{C}}i{\c{c}}ek, {\"{O}}zg{\"{u}}n and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
eprint = {1606.06650},
file = {:Users/pietz/Google Drive/Documents/Mendeley/{\c{C}}i{\c{c}}ek et al. - 2016 - 3D U-Net Learning Dense Volumetric Segmentation from Sparse Annotation.pdf:pdf},
month = {jun},
title = {{3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation}},
url = {http://arxiv.org/abs/1606.06650},
year = {2016}
}
@article{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
eprint = {1505.04597},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf},
month = {may},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
year = {2015}
}
@article{Zhang2017,
abstract = {We introduce an extremely computation efficient CNN architecture named ShuffleNet, designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two proposed operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 6.7$\backslash${\%}) than the recent MobileNet system on ImageNet classification under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves $\backslash$textasciitilde 13{\$}\backslashtimes{\$} actual speedup over AlexNet while maintaining comparable accuracy.},
archivePrefix = {arXiv},
arxivId = {1707.01083},
author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
eprint = {1707.01083},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Zhang et al. - 2017 - ShuffleNet An Extremely Efficient Convolutional Neural Network for Mobile Devices.pdf:pdf},
month = {jul},
title = {{ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices}},
url = {http://arxiv.org/abs/1707.01083},
year = {2017}
}
@article{Rajpurkar2017,
abstract = {We develop an algorithm which exceeds the performance of board certified cardiologists in detecting a wide range of heart arrhythmias from electrocardiograms recorded with a single-lead wearable monitor. We build a dataset with more than 500 times the number of unique patients than previously studied corpora. On this dataset, we train a 34-layer convolutional neural network which maps a sequence of ECG samples to a sequence of rhythm classes. Committees of board-certified cardiologists annotate a gold standard test set on which we compare the performance of our model to that of 6 other individual cardiologists. We exceed the average cardiologist performance in both recall (sensitivity) and precision (positive predictive value).},
archivePrefix = {arXiv},
arxivId = {1707.01836},
author = {Rajpurkar, Pranav and Hannun, Awni Y. and Haghpanahi, Masoumeh and Bourn, Codie and Ng, Andrew Y.},
eprint = {1707.01836},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Rajpurkar et al. - 2017 - Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks.pdf:pdf},
month = {jul},
title = {{Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1707.01836},
year = {2017}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
eprint = {1603.07285},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Dumoulin, Visin - 2016 - A guide to convolution arithmetic for deep learning.pdf:pdf},
month = {mar},
title = {{A guide to convolution arithmetic for deep learning}},
url = {http://arxiv.org/abs/1603.07285},
year = {2016}
}
@article{Ronneberger2015a,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
eprint = {1505.04597},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf},
month = {may},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
year = {2015}
}
@article{Noh2015,
abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5{\%}) among the methods trained with no external data through ensemble with the fully convolutional network.},
archivePrefix = {arXiv},
arxivId = {1505.04366},
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
eprint = {1505.04366},
month = {may},
title = {{Learning Deconvolution Network for Semantic Segmentation}},
url = {http://arxiv.org/abs/1505.04366},
year = {2015}
}
@article{Hamet2017,
abstract = {Artificial Intelligence (AI) is a general term that implies the use of a computer to model intelligent behavior with minimal human intervention. AI is generally accepted as having started with the invention of robots. The term derives from the Czech word robota, meaning biosynthetic machines used as forced labor. In this field, Leonardo Da Vinci's lasting heritage is today's burgeoning use of robotic-assisted surgery, named after him, for complex urologic and gynecologic procedures. Da Vinci's sketchbooks of robots helped set the stage for this innovation. AI, described as the science and engineering of making intelligent machines, was officially born in 1956. The term is applicable to a broad range of items in medicine such as robotics, medical diagnosis, medical statistics, and human biology???up to and including today's ???omics???. AI in medicine, which is the focus of this review, has two main branches: virtual and physical. The virtual branch includes informatics approaches from deep learning information management to control of health management systems, including electronic health records, and active guidance of physicians in their treatment decisions. The physical branch is best represented by robots used to assist the elderly patient or the attending surgeon. Also embodied in this branch are targeted nanorobots, a unique new drug delivery system. The societal and ethical complexities of these applications require further reflection, proof of their medical utility, economic value, and development of interdisciplinary strategies for their wider application.},
author = {Hamet, Pavel and Tremblay, Johanne},
doi = {10.1016/j.metabol.2017.01.011},
isbn = {978-3-642-38325-0},
issn = {15328600},
journal = {Metabolism: Clinical and Experimental},
pmid = {15333167},
title = {{Artificial intelligence in medicine}},
volume = {69},
year = {2017}
}
@article{Springenberg2014,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
eprint = {1412.6806},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Springenberg et al. - 2014 - Striving for Simplicity The All Convolutional Net.pdf:pdf},
month = {dec},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {http://arxiv.org/abs/1412.6806},
year = {2014}
}
@article{Roth2015,
abstract = {Automated classification of human anatomy is an important prerequisite for many computer-aided diagnosis systems. The spatial complexity and variability of anatomy throughout the human body makes classification difficult. "Deep learning" methods such as convolutional networks (ConvNets) outperform other state-of-the-art methods in image classification tasks. In this work, we present a method for organ- or body-part-specific anatomical classification of medical images acquired using computed tomography (CT) with ConvNets. We train a ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomical classes. Key-images were mined from a hospital PACS archive, using a set of 1,675 patients. We show that a data augmentation approach can help to enrich the data set and improve classification performance. Using ConvNets and data augmentation, we achieve anatomy-specific classification error of 5.9 {\%} and area-under-the-curve (AUC) values of an average of 0.998 in testing. We demonstrate that deep learning can be used to train very reliable and accurate classifiers that could initialize further computer-aided diagnosis.},
archivePrefix = {arXiv},
arxivId = {1504.04003},
author = {Roth, Holger R. and Lee, Christopher T. and Shin, Hoo-Chang and Seff, Ari and Kim, Lauren and Yao, Jianhua and Lu, Le and Summers, Ronald M.},
doi = {10.1109/ISBI.2015.7163826},
eprint = {1504.04003},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Roth et al. - 2015 - Anatomy-specific classification of medical images using deep convolutional nets.pdf:pdf},
month = {apr},
title = {{Anatomy-specific classification of medical images using deep convolutional nets}},
url = {http://arxiv.org/abs/1504.04003 http://dx.doi.org/10.1109/ISBI.2015.7163826},
year = {2015}
}
@article{Camlica2015,
abstract = {Good results on image classification and retrieval using support vector machines (SVM) with local binary patterns (LBPs) as features have been extensively reported in the literature where an entire image is retrieved or classified. In contrast, in medical imaging, not all parts of the image may be equally significant or relevant to the image retrieval application at hand. For instance, in lung x-ray image, the lung region may contain a tumour, hence being highly significant whereas the surrounding area does not contain significant information from medical diagnosis perspective. In this paper, we propose to detect salient regions of images during training and fold the data to reduce the effect of irrelevant regions. As a result, smaller image areas will be used for LBP features calculation and consequently classification by SVM. We use IRMA 2009 dataset with 14,410 x-ray images to verify the performance of the proposed approach. The results demonstrate the benefits of saliency-based folding approach that delivers comparable classification accuracies with state-of-the-art but exhibits lower computational cost and storage requirements, factors highly important for big data analytics.},
archivePrefix = {arXiv},
arxivId = {1509.04619},
author = {Camlica, Zehra and Tizhoosh, H. R. and Khalvati, Farzad},
doi = {10.1109/ICMLA.2015.131},
eprint = {1509.04619},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Camlica, Tizhoosh, Khalvati - 2015 - Medical Image Classification via SVM using LBP Features from Saliency-Based Folded Data.pdf:pdf},
month = {sep},
title = {{Medical Image Classification via SVM using LBP Features from Saliency-Based Folded Data}},
url = {http://arxiv.org/abs/1509.04619 http://dx.doi.org/10.1109/ICMLA.2015.131},
year = {2015}
}
@article{Cho2015,
abstract = {The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems.},
archivePrefix = {arXiv},
arxivId = {1511.06348},
author = {Cho, Junghwan and Lee, Kyewook and Shin, Ellie and Choy, Garry and Do, Synho},
eprint = {1511.06348},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Cho et al. - 2015 - How much data is needed to train a medical image deep learning system to achieve necessary high accuracy.pdf:pdf},
month = {nov},
title = {{How much data is needed to train a medical image deep learning system to achieve necessary high accuracy?}},
url = {http://arxiv.org/abs/1511.06348},
year = {2015}
}
@article{Mishkin2016,
abstract = {The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the "deficit" is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.},
archivePrefix = {arXiv},
arxivId = {1606.02228},
author = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
doi = {10.1016/j.cviu.2017.05.007},
eprint = {1606.02228},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Mishkin, Sergievskiy, Matas - 2016 - Systematic evaluation of CNN advances on the ImageNet.pdf:pdf},
month = {jun},
title = {{Systematic evaluation of CNN advances on the ImageNet}},
url = {http://arxiv.org/abs/1606.02228 http://dx.doi.org/10.1016/j.cviu.2017.05.007},
year = {2016}
}
@article{Iandola2016,
abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
archivePrefix = {arXiv},
arxivId = {1602.07360},
author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
eprint = {1602.07360},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer parameters and.pdf:pdf},
month = {feb},
title = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and}},
url = {http://arxiv.org/abs/1602.07360},
year = {2016}
}
@article{Anderson2016,
abstract = {In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of domain-specific hand-engineered features. Here we take the approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.},
archivePrefix = {arXiv},
arxivId = {1611.01714},
author = {Anderson, Ark and Shaffer, Kyle and Yankov, Artem and Corley, Court D. and Hodas, Nathan O.},
eprint = {1611.01714},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Anderson et al. - 2016 - Beyond Fine Tuning A Modular Approach to Learning on Small Data.pdf:pdf},
month = {nov},
title = {{Beyond Fine Tuning: A Modular Approach to Learning on Small Data}},
url = {http://arxiv.org/abs/1611.01714},
year = {2016}
}
@article{Cole2016,
abstract = {Machine learning analysis of neuroimaging data can accurately predict chronological age in healthy people and deviations from healthy brain ageing have been associated with cognitive impairment and disease. Here we sought to further establish the credentials of "brain-predicted age" as a biomarker of individual differences in the brain ageing process, using a predictive modelling approach based on deep learning, and specifically convolutional neural networks (CNN), and applied to both pre-processed and raw T1-weighted MRI data. Firstly, we aimed to demonstrate the accuracy of CNN brain-predicted age using a large dataset of healthy adults (N = 2001). Next, we sought to establish the heritability of brain-predicted age using a sample of monozygotic and dizygotic female twins (N = 62). Thirdly, we examined the test-retest and multi-centre reliability of brain-predicted age using two samples (within-scanner N = 20; between-scanner N = 11). CNN brain-predicted ages were generated and compared to a Gaussian Process Regression (GPR) approach, on all datasets. Input data were grey matter (GM) or white matter (WM) volumetric maps generated by Statistical Parametric Mapping (SPM) or raw data. Brain-predicted age represents an accurate, highly reliable and genetically-valid phenotype, that has potential to be used as a biomarker of brain ageing. Moreover, age predictions can be accurately generated on raw T1-MRI data, substantially reducing computation time for novel data, bringing the process closer to giving real-time information on brain health in clinical settings.},
archivePrefix = {arXiv},
arxivId = {1612.02572},
author = {Cole, James H and Poudel, Rudra PK and Tsagkrasoulis, Dimosthenis and Caan, Matthan WA and Steves, Claire and Spector, Tim D and Montana, Giovanni},
eprint = {1612.02572},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Cole et al. - 2016 - Predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker.pdf:pdf},
month = {dec},
title = {{Predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker}},
url = {http://arxiv.org/abs/1612.02572},
year = {2016}
}
@article{Wu2017,
abstract = {Recently, DNN model compression based on network architecture design, e.g., SqueezeNet, attracted a lot attention. No accuracy drop on image classification is observed on these extremely compact networks, compared to well-known models. An emerging question, however, is whether these model compression techniques hurt DNN's learning ability other than classifying images on a single dataset. Our preliminary experiment shows that these compression methods could degrade domain adaptation (DA) ability, though the classification performance is preserved. Therefore, we propose a new compact network architecture and unsupervised DA method in this paper. The DNN is built on a new basic module Conv-M which provides more diverse feature extractors without significantly increasing parameters. The unified framework of our DA method will simultaneously learn invariance across domains, reduce divergence of feature representations, and adapt label prediction. Our DNN has 4.1M parameters, which is only 6.7{\%} of AlexNet or 59{\%} of GoogLeNet. Experiments show that our DNN obtains GoogLeNet-level accuracy both on classification and DA, and our DA method slightly outperforms previous competitive ones. Put all together, our DA strategy based on our DNN achieves state-of-the-art on sixteen of total eighteen DA tasks on popular Office-31 and Office-Caltech datasets.},
archivePrefix = {arXiv},
arxivId = {1703.04071},
author = {Wu, Chunpeng and Wen, Wei and Afzal, Tariq and Zhang, Yongmei and Chen, Yiran and Li, Hai},
eprint = {1703.04071},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Wu et al. - 2017 - A Compact DNN Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation.pdf:pdf},
month = {mar},
title = {{A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation}},
url = {http://arxiv.org/abs/1703.04071},
year = {2017}
}
@article{Smith2015,
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
archivePrefix = {arXiv},
arxivId = {1506.01186},
author = {Smith, Leslie N.},
eprint = {1506.01186},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Smith - 2015 - Cyclical Learning Rates for Training Neural Networks.pdf:pdf},
month = {jun},
title = {{Cyclical Learning Rates for Training Neural Networks}},
url = {http://arxiv.org/abs/1506.01186},
year = {2015}
}
@article{Xu2017,
abstract = {Image matting is a fundamental computer vision problem and has many applications. Previous algorithms have poor performance when an image has similar foreground and background colors or complicated textures. The main reasons are prior methods 1) only use low-level features and 2) lack high-level context. In this paper, we propose a novel deep learning based algorithm that can tackle both these problems. Our deep model has two parts. The first part is a deep convolutional encoder-decoder network that takes an image and the corresponding trimap as inputs and predict the alpha matte of the image. The second part is a small convolutional network that refines the alpha matte predictions of the first network to have more accurate alpha values and sharper edges. In addition, we also create a large-scale image matting dataset including 49300 training images and 1000 testing images. We evaluate our algorithm on the image matting benchmark, our testing set, and a wide variety of real images. Experimental results clearly demonstrate the superiority of our algorithm over previous methods.},
archivePrefix = {arXiv},
arxivId = {1703.03872},
author = {Xu, Ning and Price, Brian and Cohen, Scott and Huang, Thomas},
eprint = {1703.03872},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Xu et al. - 2017 - Deep Image Matting.pdf:pdf},
month = {mar},
title = {{Deep Image Matting}},
url = {http://arxiv.org/abs/1703.03872},
year = {2017}
}
@article{Ravishankar2017,
abstract = {The ability to automatically learn task specific feature representations has led to a huge success of deep learning methods. When large training data is scarce, such as in medical imaging problems, transfer learning has been very effective. In this paper, we systematically investigate the process of transferring a Convolutional Neural Network, trained on ImageNet images to perform image classification, to kidney detection problem in ultrasound images. We study how the detection performance depends on the extent of transfer. We show that a transferred and tuned CNN can outperform a state-of-the-art feature engineered pipeline and a hybridization of these two techniques achieves 20$\backslash${\%} higher performance. We also investigate how the evolution of intermediate response images from our network. Finally, we compare these responses to state-of-the-art image processing filters in order to gain greater insight into how transfer learning is able to effectively manage widely varying imaging regimes.},
archivePrefix = {arXiv},
arxivId = {1704.06040},
author = {Ravishankar, Hariharan and Sudhakar, Prasad and Venkataramani, Rahul and Thiruvenkadam, Sheshadri and Annangi, Pavan and Babu, Narayanan and Vaidya, Vivek},
eprint = {1704.06040},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Ravishankar et al. - 2017 - Understanding the Mechanisms of Deep Transfer Learning for Medical Images.pdf:pdf},
month = {apr},
title = {{Understanding the Mechanisms of Deep Transfer Learning for Medical Images}},
url = {http://arxiv.org/abs/1704.06040},
year = {2017}
}
@article{Razzak2017,
abstract = {Healthcare sector is totally different from other industry. It is on high priority sector and people expect highest level of care and services regardless of cost. It did not achieve social expectation even though it consume huge percentage of budget. Mostly the interpretations of medical data is being done by medical expert. In terms of image interpretation by human expert, it is quite limited due to its subjectivity, the complexity of the image, extensive variations exist across different interpreters, and fatigue. After the success of deep learning in other real world application, it is also providing exciting solutions with good accuracy for medical imaging and is seen as a key method for future applications in health secotr. In this chapter, we discussed state of the art deep learning architecture and its optimization used for medical image segmentation and classification. In the last section, we have discussed the challenges deep learning based methods for medical imaging and open research issue.},
archivePrefix = {arXiv},
arxivId = {1704.06825},
author = {Razzak, Muhammad Imran and Naz, Saeeda and Zaib, Ahmad},
eprint = {1704.06825},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Razzak, Naz, Zaib - 2017 - Deep Learning for Medical Image Processing Overview, Challenges and Future.pdf:pdf},
month = {apr},
title = {{Deep Learning for Medical Image Processing: Overview, Challenges and Future}},
url = {http://arxiv.org/abs/1704.06825},
year = {2017}
}
@article{Shwartz-Ziv2017,
abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the $\backslash$textit{\{}Information Plane{\}}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on {\{}$\backslash$emph compression{\}} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
archivePrefix = {arXiv},
arxivId = {1703.00810},
author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
eprint = {1703.00810},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Shwartz-Ziv, Tishby - 2017 - Opening the Black Box of Deep Neural Networks via Information.pdf:pdf},
month = {mar},
title = {{Opening the Black Box of Deep Neural Networks via Information}},
url = {http://arxiv.org/abs/1703.00810},
year = {2017}
}
@article{Qassim2017,
abstract = {Deep learning has given way to a new era of machine learning, apart from computer vision. Convolutional neural networks have been implemented in image classification, segmentation and object detection. Despite recent advancements, we are still in the very early stages and have yet to settle on best practices for network architecture in terms of deep design, small in size and a short training time. In this work, we propose a very deep neural network comprised of 16 Convolutional layers compressed with the Fire Module adapted from the SQUEEZENET model. We also call for the addition of residual connections to help suppress degradation. This model can be implemented on almost every neural network model with fully incorporated residual learning. This proposed model Residual-Squeeze-VGG16 (ResSquVGG16) trained on the large-scale MIT Places365-Standard scene dataset. In our tests, the model performed with accuracy similar to the pre-trained VGG16 model in Top-1 and Top-5 validation accuracy while also enjoying a 23.86{\%} reduction in training time and an 88.4{\%} reduction in size. In our tests, this model was trained from scratch.},
archivePrefix = {arXiv},
arxivId = {1705.03004},
author = {Qassim, Hussam and Feinzimer, David and Verma, Abhishek},
eprint = {1705.03004},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Qassim, Feinzimer, Verma - 2017 - Residual Squeeze VGG16.pdf:pdf},
month = {may},
title = {{Residual Squeeze VGG16}},
url = {http://arxiv.org/abs/1705.03004},
year = {2017}
}
@article{Sarraf2016,
abstract = {Recently, machine learning techniques especially predictive modeling and pattern recognition in biomedical sciences from drug delivery system to medical imaging has become one of the important methods which are assisting researchers to have deeper understanding of entire issue and to solve complex medical problems. Deep learning is a powerful machine learning algorithm in classification while extracting low to high-level features. In this paper, we used convolutional neural network to classify Alzheimer's brain from normal healthy brain. The importance of classifying this kind of medical data is to potentially develop a predict model or system in order to recognize the type disease from normal subjects or to estimate the stage of the disease. Classification of clinical data such as Alzheimer's disease has been always challenging and most problematic part has been always selecting the most discriminative features. Using Convolutional Neural Network (CNN) and the famous architecture LeNet-5, we successfully classified structural MRI data of Alzheimer's subjects from normal controls where the accuracy of test data on trained data reached 98.84{\%}. This experiment suggests us the shift and scale invariant features extracted by CNN followed by deep learning classification is most powerful method to distinguish clinical data from healthy data in fMRI. This approach also enables us to expand our methodology to predict more complicated systems.},
archivePrefix = {arXiv},
arxivId = {1607.06583},
author = {Sarraf, Saman and Tofighi, Ghassem},
eprint = {1607.06583},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Sarraf, Tofighi - 2016 - Classification of Alzheimer's Disease Structural MRI Data by Deep Learning Convolutional Neural Networks.pdf:pdf},
month = {jul},
title = {{Classification of Alzheimer's Disease Structural MRI Data by Deep Learning Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1607.06583},
year = {2016}
}
@article{Grace2017,
abstract = {Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50{\%} chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.},
archivePrefix = {arXiv},
arxivId = {1705.08807},
author = {Grace, Katja and Salvatier, John and Dafoe, Allan and Zhang, Baobao and Evans, Owain},
eprint = {1705.08807},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Grace et al. - 2017 - When Will AI Exceed Human Performance Evidence from AI Experts.pdf:pdf},
month = {may},
title = {{When Will AI Exceed Human Performance? Evidence from AI Experts}},
url = {http://arxiv.org/abs/1705.08807},
year = {2017}
}
@article{Rolnick2017,
abstract = {Deep neural networks trained on large supervised datasets have led to impressive results in recent years. However, since well-annotated datasets can be prohibitively expensive and time-consuming to collect, recent work has explored the use of larger but noisy datasets that can be more easily obtained. In this paper, we investigate the behavior of deep neural networks on training sets with massively noisy labels. We show that successful learning is possible even with an essentially arbitrary amount of noise. For example, on MNIST we find that accuracy of above 90 percent is still attainable even when the dataset has been diluted with 100 noisy examples for each clean example. Such behavior holds across multiple patterns of label noise, even when noisy labels are biased towards confusing classes. Further, we show how the required dataset size for successful training increases with higher label noise. Finally, we present simple actionable techniques for improving learning in the regime of high label noise.},
archivePrefix = {arXiv},
arxivId = {1705.10694},
author = {Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
eprint = {1705.10694},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Rolnick et al. - 2017 - Deep Learning is Robust to Massive Label Noise.pdf:pdf},
month = {may},
title = {{Deep Learning is Robust to Massive Label Noise}},
url = {http://arxiv.org/abs/1705.10694},
year = {2017}
}
@article{Tajbakhsh2017,
abstract = {Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: $\backslash$emph{\{}Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch?{\}} To address this question, we considered 4 distinct medical imaging applications in 3 specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from 3 different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that (1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; (2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; (3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and (4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.},
archivePrefix = {arXiv},
arxivId = {1706.00712},
author = {Tajbakhsh, Nima and Shin, Jae Y. and Gurudu, Suryakanth R. and Hurst, R. Todd and Kendall, Christopher B. and Gotway, Michael B. and Liang, Jianming},
doi = {10.1109/TMI.2016.2535302},
eprint = {1706.00712},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Tajbakhsh et al. - 2017 - Convolutional Neural Networks for Medical Image Analysis Full Training or Fine Tuning.pdf:pdf},
month = {jun},
title = {{Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?}},
url = {http://arxiv.org/abs/1706.00712 http://dx.doi.org/10.1109/TMI.2016.2535302},
year = {2017}
}
@article{Dong2017,
abstract = {A major challenge in brain tumor treatment planning and quantitative evaluation is determination of the tumor extent. The noninvasive magnetic resonance imaging (MRI) technique has emerged as a front-line diagnostic tool for brain tumors without ionizing radiation. Manual segmentation of brain tumor extent from 3D MRI volumes is a very time-consuming task and the performance is highly relied on operator's experience. In this context, a reliable fully automatic segmentation method for the brain tumor segmentation is necessary for an efficient measurement of the tumor extent. In this study, we propose a fully automatic method for brain tumor segmentation, which is developed using U-Net based deep convolutional networks. Our method was evaluated on Multimodal Brain Tumor Image Segmentation (BRATS 2015) datasets, which contain 220 high-grade brain tumor and 54 low-grade tumor cases. Cross-validation has shown that our method can obtain promising segmentation efficiently.},
archivePrefix = {arXiv},
arxivId = {1705.03820},
author = {Dong, Hao and Yang, Guang and Liu, Fangde and Mo, Yuanhan and Guo, Yike},
eprint = {1705.03820},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Dong et al. - 2017 - Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks.pdf:pdf},
month = {may},
title = {{Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks}},
url = {http://arxiv.org/abs/1705.03820},
year = {2017}
}
@article{Litjens2017,
abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks and provide concise overviews of studies per application area. Open challenges and directions for future research are discussed.},
archivePrefix = {arXiv},
arxivId = {1702.05747},
author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A. W. M. and van Ginneken, Bram and S{\'{a}}nchez, Clara I.},
eprint = {1702.05747},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Litjens et al. - 2017 - A Survey on Deep Learning in Medical Image Analysis.pdf:pdf},
month = {feb},
title = {{A Survey on Deep Learning in Medical Image Analysis}},
url = {http://arxiv.org/abs/1702.05747},
year = {2017}
}
@article{Salehi2017,
abstract = {Fully convolutional deep neural networks carry out excellent potential for fast and accurate image segmentation. One of the main challenges in training these networks is data imbalance, which is particularly problematic in medical imaging applications such as lesion segmentation where the number of lesion voxels is often much lower than the number of non-lesion voxels. Training with unbalanced data can lead to predictions that are severely biased towards high precision but low recall (sensitivity), which is undesired especially in medical applications where false negatives are much less tolerable than false positives. Several methods have been proposed to deal with this problem including balanced sampling, two step training, sample re-weighting, and similarity loss functions. In this paper, we propose a generalized loss function based on the Tversky index to address the issue of data imbalance and achieve much better trade-off between precision and recall in training 3D fully convolutional deep neural networks. Experimental results in multiple sclerosis lesion segmentation on magnetic resonance images show improved F2 score, Dice coefficient, and the area under the precision-recall curve in test data. Based on these results we suggest Tversky loss function as a generalized framework to effectively train deep neural networks.},
archivePrefix = {arXiv},
arxivId = {1706.05721},
author = {Salehi, Seyed Sadegh Mohseni and Erdogmus, Deniz and Gholipour, Ali},
eprint = {1706.05721},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Salehi, Erdogmus, Gholipour - 2017 - Tversky loss function for image segmentation using 3D fully convolutional deep networks.pdf:pdf},
month = {jun},
title = {{Tversky loss function for image segmentation using 3D fully convolutional deep networks}},
url = {http://arxiv.org/abs/1706.05721},
year = {2017}
}
@article{Klambauer2017,
abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
archivePrefix = {arXiv},
arxivId = {1706.02515},
author = {Klambauer, G{\"{u}}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
eprint = {1706.02515},
month = {jun},
title = {{Self-Normalizing Neural Networks}},
url = {http://arxiv.org/abs/1706.02515},
year = {2017}
}
@article{Czarnecki2017,
abstract = {At the heart of deep learning we aim to use neural networks as function approximators - training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input - for example when the ground truth function is itself a neural network such as in network compression or distillation. Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the function's outputs but also the function's derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients. In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.},
archivePrefix = {arXiv},
arxivId = {1706.04859},
author = {Czarnecki, Wojciech Marian and Osindero, Simon and Jaderberg, Max and {\'{S}}wirszcz, Grzegorz and Pascanu, Razvan},
eprint = {1706.04859},
month = {jun},
title = {{Sobolev Training for Neural Networks}},
url = {http://arxiv.org/abs/1706.04859},
year = {2017}
}
@article{Wang2017a,
abstract = {Accurate medical image segmentation is essential for diagnosis, surgical planning and many other applications. Convolutional Neural Networks (CNNs) have shown to be state-of-the-art automatic segmentation methods while the result still needs to be refined to become accurate and robust enough for clinical use. We propose a deep learning-based interactive segmentation method in order to improve the segmentation obtained by an automatic CNN as well as reduce user interactions during refinement for better results. We use one CNN to obtain an initial segmentation automatically, on which user interactions are added to indicate mis-segmentations. Another CNN takes as input the user interactions with the initial segmentation and gives a refined result. We propose a new way to combine user interactions with CNNs through geodesic distance maps, and propose a resolution-preserving network that can give better dense prediction. In addition, we integrate user interactions as hard constraints into back-propagatable Conditional Random Fields. We validated the proposed framework in the application of placenta segmentation from fetal MRI and clavicle segmentation from chest radiographs. Experimental results show our method achieves a large improvement from automatic CNNs, and obtains comparable accuracy with fewer user interventions and less time compared with traditional interactive methods.},
archivePrefix = {arXiv},
arxivId = {1707.00652},
author = {Wang, Guotai and Zuluaga, Maria A. and Li, Wenqi and Pratt, Rosalind and Patel, Premal A. and Aertsen, Michael and Doel, Tom and David, Anna L. and Deprest, Jan and Ourselin, Sebastien and Vercauteren, Tom},
eprint = {1707.00652},
file = {:Users/pietz/Google Drive/Documents/Mendeley/Wang et al. - 2017 - DeepIGeoS A Deep Interactive Geodesic Framework for Medical Image Segmentation.pdf:pdf},
month = {jul},
title = {{DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation}},
url = {http://arxiv.org/abs/1707.00652},
year = {2017}
}
